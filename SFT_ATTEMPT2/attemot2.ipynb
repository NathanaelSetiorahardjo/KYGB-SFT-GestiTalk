{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aded3844",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088888f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (250, 20, 96, 96, 3) (250,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "DATASET_DIR = \"D:/SFT_ATTEMPT2/raw_video\"  # <-- adjust full path to your dataset\n",
    "TARGET_WORDS = [\"Halo\", \"Kamu\", \"Apa\", \"Dimana\", \"Duduk\"]\n",
    "IMG_SIZE = 96\n",
    "FRAMES_PER_VIDEO = 20  # sample fixed length from each video\n",
    "\n",
    "def load_videos():\n",
    "    X, y = [], []\n",
    "    for word in TARGET_WORDS:\n",
    "        word_path = os.path.join(DATASET_DIR, word)\n",
    "        for vid_file in os.listdir(word_path):\n",
    "            vid_path = os.path.join(word_path, vid_file)\n",
    "            cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "            frames = []\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = frame.astype(\"float32\") / 255.0\n",
    "                frames.append(frame)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "            # Uniformly sample FRAMES_PER_VIDEO frames\n",
    "            if len(frames) >= FRAMES_PER_VIDEO:\n",
    "                idxs = np.linspace(0, len(frames)-1, FRAMES_PER_VIDEO, dtype=int)\n",
    "                frames = [frames[i] for i in idxs]\n",
    "                X.append(frames)\n",
    "                y.append(TARGET_WORDS.index(word))\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = load_videos()\n",
    "print(\"Shape:\", X.shape, y.shape)  # (num_samples, FRAMES_PER_VIDEO, 96, 96, 3)\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d13ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Halo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:55<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Kamu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:39<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Apa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:33<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Dimana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:45<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Duduk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:20<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset shapes: (773, 20, 128) (773,)\n",
      "Saved to cache_landmarks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess.py\n",
    "import os, glob, json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_DIR = \"D:/SFT_ATTEMPT2/raw_video\"\n",
    "TARGET_WORDS = [\"Halo\",\"Kamu\",\"Apa\",\"Dimana\",\"Duduk\"]\n",
    "CACHE_DIR = \"cache_landmarks\"\n",
    "SEQ_LEN = 20\n",
    "TARGET_FPS = 24\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, model_complexity=0,\n",
    "                       max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "def normalize_hand(landmarks, w, h):\n",
    "    \"\"\"Return 63-d vector for 21 landmarks (x,y,z) normalized relative to wrist & scale.\n",
    "       If landmarks is None return zeros.\"\"\"\n",
    "    if landmarks is None:\n",
    "        return np.zeros(63, dtype=np.float32)\n",
    "    pts = np.array([(lm.x, lm.y, lm.z) for lm in landmarks], dtype=np.float32)\n",
    "    # center at wrist (index 0)\n",
    "    center = pts[0].copy()\n",
    "    pts[:, :2] -= center[:2]\n",
    "    px = pts[:,0]*w\n",
    "    py = pts[:,1]*h\n",
    "    scale = max(px.max()-px.min(), py.max()-py.min(), 1e-3)\n",
    "    pts[:, :2] /= (scale / max(w,h))\n",
    "    return pts.flatten().astype(np.float32)\n",
    "\n",
    "def extract_sequences_from_video(path, seq_len=SEQ_LEN, target_fps=TARGET_FPS):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        return []\n",
    "    src_fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    step = max(int(round(src_fps / target_fps)), 1)\n",
    "\n",
    "    frames_feats = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if i % step != 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        h,w = frame.shape[:2]\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = hands.process(rgb)\n",
    "\n",
    "        left = None; right = None\n",
    "        if res.multi_hand_landmarks and res.multi_handedness:\n",
    "            # pair them\n",
    "            for lm, handed in zip(res.multi_hand_landmarks, res.multi_handedness):\n",
    "                if handed.classification[0].label.lower() == \"left\":\n",
    "                    left = lm.landmark\n",
    "                else:\n",
    "                    right = lm.landmark\n",
    "\n",
    "        L = normalize_hand(left, w, h)\n",
    "        R = normalize_hand(right, w, h)\n",
    "        present = np.array([1.0 if left is not None else 0.0, 1.0 if right is not None else 0.0], dtype=np.float32)\n",
    "        feat = np.concatenate([L, R, present])    # length = 63+63+2 = 128\n",
    "        frames_feats.append(feat)\n",
    "        i += 1\n",
    "\n",
    "    cap.release()\n",
    "    if len(frames_feats) < seq_len:\n",
    "        return []\n",
    "\n",
    "    seqs = []\n",
    "    for start in range(0, len(frames_feats) - seq_len + 1, seq_len):\n",
    "        seqs.append(np.stack(frames_feats[start:start+seq_len], axis=0))\n",
    "    return seqs\n",
    "\n",
    "# Scan dataset\n",
    "X=[]; y=[]\n",
    "label_map = {w:i for i,w in enumerate(TARGET_WORDS)}\n",
    "for w in TARGET_WORDS:\n",
    "    folder = os.path.join(DATASET_DIR, w)\n",
    "    if not os.path.isdir(folder): \n",
    "        print(\"Missing folder:\", folder); continue\n",
    "    vids = glob.glob(os.path.join(folder, \"*.mp4\"))\n",
    "    print(f\"Found {len(vids)} videos for {w}\")\n",
    "    for v in tqdm(vids):\n",
    "        seqs = extract_sequences_from_video(v)\n",
    "        for s in seqs:\n",
    "            X.append(s)   # shape (T,128)\n",
    "            y.append(label_map[w])\n",
    "\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.int64)\n",
    "\n",
    "print(\"Built dataset shapes:\", X.shape, y.shape)\n",
    "np.save(os.path.join(CACHE_DIR, \"X.npy\"), X)\n",
    "np.save(os.path.join(CACHE_DIR, \"y.npy\"), y)\n",
    "with open(os.path.join(CACHE_DIR, \"labels.json\"), \"w\") as f:\n",
    "    json.dump(TARGET_WORDS, f)\n",
    "print(\"Saved to\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84dcd400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shapes: (773, 20, 128) (773,)\n",
      "Train: (618, 20, 128) (618,) Val: (155, 20, 128) (155,)\n",
      "Class weights: {0: 0.8524137931034482, 1: 1.0747826086956522, 2: 1.0564102564102564, 3: 1.03, 4: 1.0214876033057851}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_81\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_81\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ masking_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ any_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ any_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_1 (\u001b[38;5;33mMasking\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any_1 (\u001b[38;5;33mAny\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m131,584\u001b[0m │ masking_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ any_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m49,408\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ any_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m325\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,477</span> (724.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m185,477\u001b[0m (724.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,477</span> (724.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m185,477\u001b[0m (724.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 20\n",
      "Epoch 1/80\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.81290, saving model to bisindo_landmarks.keras\n",
      "20/20 - 5s - 254ms/step - accuracy: 0.6440 - loss: 1.1415 - val_accuracy: 0.8129 - val_loss: 0.6547 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.81290 to 0.87097, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.8738 - loss: 0.4228 - val_accuracy: 0.8710 - val_loss: 0.3642 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.87097 to 0.89032, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 21ms/step - accuracy: 0.9207 - loss: 0.2708 - val_accuracy: 0.8903 - val_loss: 0.3343 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.89032 to 0.89677, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 23ms/step - accuracy: 0.9482 - loss: 0.1781 - val_accuracy: 0.8968 - val_loss: 0.2977 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.89677 to 0.90323, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 24ms/step - accuracy: 0.9466 - loss: 0.1700 - val_accuracy: 0.9032 - val_loss: 0.2771 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.90323 to 0.90968, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9595 - loss: 0.1518 - val_accuracy: 0.9097 - val_loss: 0.2695 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.90968\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9466 - loss: 0.1675 - val_accuracy: 0.8968 - val_loss: 0.3032 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.90968\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9191 - loss: 0.2215 - val_accuracy: 0.9097 - val_loss: 0.2713 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.90968\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9612 - loss: 0.1234 - val_accuracy: 0.9032 - val_loss: 0.2566 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.90968 to 0.92258, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9693 - loss: 0.0999 - val_accuracy: 0.9226 - val_loss: 0.2243 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.92258\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9628 - loss: 0.0965 - val_accuracy: 0.9097 - val_loss: 0.2329 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.92258 to 0.92903, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9660 - loss: 0.0897 - val_accuracy: 0.9290 - val_loss: 0.2167 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.92903 to 0.93548, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 21ms/step - accuracy: 0.9741 - loss: 0.0815 - val_accuracy: 0.9355 - val_loss: 0.2169 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9693 - loss: 0.0835 - val_accuracy: 0.9355 - val_loss: 0.2127 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 19ms/step - accuracy: 0.9725 - loss: 0.0819 - val_accuracy: 0.9355 - val_loss: 0.2096 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 19ms/step - accuracy: 0.9741 - loss: 0.0769 - val_accuracy: 0.9161 - val_loss: 0.2291 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9725 - loss: 0.0711 - val_accuracy: 0.9290 - val_loss: 0.2355 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.93548\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "20/20 - 0s - 21ms/step - accuracy: 0.9741 - loss: 0.0682 - val_accuracy: 0.9355 - val_loss: 0.2178 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 21ms/step - accuracy: 0.9757 - loss: 0.0614 - val_accuracy: 0.9355 - val_loss: 0.1991 - learning_rate: 5.0000e-04\n",
      "Epoch 20/80\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9757 - loss: 0.0651 - val_accuracy: 0.9355 - val_loss: 0.2151 - learning_rate: 5.0000e-04\n",
      "Epoch 21/80\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9757 - loss: 0.0638 - val_accuracy: 0.9355 - val_loss: 0.2116 - learning_rate: 5.0000e-04\n",
      "Epoch 22/80\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.93548\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9741 - loss: 0.0610 - val_accuracy: 0.9355 - val_loss: 0.2185 - learning_rate: 5.0000e-04\n",
      "Epoch 23/80\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 19ms/step - accuracy: 0.9773 - loss: 0.0561 - val_accuracy: 0.9355 - val_loss: 0.2194 - learning_rate: 2.5000e-04\n",
      "Epoch 24/80\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9741 - loss: 0.0603 - val_accuracy: 0.9290 - val_loss: 0.2128 - learning_rate: 2.5000e-04\n",
      "Epoch 25/80\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.93548\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9741 - loss: 0.0598 - val_accuracy: 0.9290 - val_loss: 0.2164 - learning_rate: 2.5000e-04\n",
      "Epoch 26/80\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 19ms/step - accuracy: 0.9838 - loss: 0.0529 - val_accuracy: 0.9290 - val_loss: 0.2144 - learning_rate: 1.2500e-04\n",
      "Epoch 27/80\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9773 - loss: 0.0556 - val_accuracy: 0.9290 - val_loss: 0.2110 - learning_rate: 1.2500e-04\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Training finished. Model saved to bisindo_landmarks.keras\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "CACHE_DIR = \"cache_landmarks\"\n",
    "SEQ_LEN = 20\n",
    "MODEL_PATH = \"bisindo_landmarks.keras\"   # recommended Keras v3 format\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 80\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "X = np.load(os.path.join(CACHE_DIR, \"X.npy\"))  # expected shape (N, T, F)\n",
    "y = np.load(os.path.join(CACHE_DIR, \"y.npy\"))\n",
    "with open(os.path.join(CACHE_DIR, \"labels.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "print(\"Loaded data shapes:\", X.shape, y.shape)\n",
    "num_classes = len(labels)\n",
    "\n",
    "# -------------------------\n",
    "# Train / val split\n",
    "# -------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train:\", X_train.shape, y_train.shape, \"Val:\", X_val.shape, y_val.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Augmentation helper\n",
    "# -------------------------\n",
    "def augment_batch(X_batch, prob=0.5):\n",
    "    \"\"\"Add small gaussian jitter to landmarks with probability per sample.\"\"\"\n",
    "    Xb = X_batch.copy()\n",
    "    for i in range(len(Xb)):\n",
    "        if np.random.rand() < prob:\n",
    "            noise = np.random.normal(0, 0.01, Xb[i].shape).astype(np.float32)\n",
    "            Xb[i] = Xb[i] + noise\n",
    "    return Xb\n",
    "\n",
    "# -------------------------\n",
    "# Compute class weights and map to sample weights\n",
    "# -------------------------\n",
    "class_weights_arr = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights_arr))\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# -------------------------\n",
    "# Generator that yields (Xb, yb, sample_weight)\n",
    "# -------------------------\n",
    "def gen_with_weights(Xa, ya, batch_size=BATCH_SIZE):\n",
    "    n = len(Xa)\n",
    "    idx = np.arange(n)\n",
    "    while True:\n",
    "        np.random.shuffle(idx)\n",
    "        for i in range(0, n, batch_size):\n",
    "            b = idx[i:i+batch_size]\n",
    "            Xb = Xa[b].copy()\n",
    "            Xb = augment_batch(Xb, prob=0.5)\n",
    "            yb = ya[b]\n",
    "            # sample weight per item based on its class\n",
    "            sw = np.array([class_weights[int(lbl)] for lbl in yb], dtype=np.float32)\n",
    "            yield Xb, yb, sw\n",
    "\n",
    "# -------------------------\n",
    "# Build model (LSTM on landmarks)\n",
    "# -------------------------\n",
    "input_shape = X_train.shape[1:]  # (T, F)\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = keras.layers.Masking(mask_value=0.0)(inputs)\n",
    "x = keras.layers.LSTM(128, return_sequences=True)(x)\n",
    "x = keras.layers.Dropout(0.25)(x)\n",
    "x = keras.layers.LSTM(64)(x)\n",
    "x = keras.layers.Dropout(0.25)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# Callbacks\n",
    "# -------------------------\n",
    "cb = [\n",
    "    keras.callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor=\"val_accuracy\", mode=\"max\", verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Fit using generator that yields sample weights\n",
    "# -------------------------\n",
    "steps_per_epoch = max(1, math.ceil(len(X_train) / BATCH_SIZE))\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "\n",
    "history = model.fit(\n",
    "    gen_with_weights(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cb,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ensure final save in Keras format\n",
    "model.save(MODEL_PATH)\n",
    "print(\"Training finished. Model saved to\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26c8b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough frames or missing landmarks\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np, json, tensorflow as tf\n",
    "from pathlib import Path\n",
    "# assumes you have the normalize function from preprocess.py (or reuse code)\n",
    "\n",
    "# quick helper: extract first seq of length SEQ_LEN from a video using mediapipe landmarks\n",
    "def extract_seq_from_video(video_path, seq_len=20, target_fps=24):\n",
    "    import mediapipe as mp\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2,\n",
    "                           min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    feats=[]\n",
    "    i=0\n",
    "    src_fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    step = max(int(round(src_fps/target_fps)),1)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        if i % step != 0:\n",
    "            i+=1; continue\n",
    "        h,w = frame.shape[:2]\n",
    "        res = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        left=None; right=None\n",
    "        if res.multi_hand_landmarks and res.multi_handedness:\n",
    "            for lm, handed in zip(res.multi_hand_landmarks, res.multi_handedness):\n",
    "                if handed.classification[0].label.lower()==\"left\": left=lm.landmark\n",
    "                else: right=lm.landmark\n",
    "        # use normalize_hand function from preprocess.py (copy it here)\n",
    "        def normalize_hand(landmarks,w,h):\n",
    "            if landmarks is None: return np.zeros(63,dtype=np.float32)\n",
    "            pts=np.array([(lm.x,lm.y,lm.z) for lm in landmarks],dtype=np.float32)\n",
    "            center=pts[0].copy(); pts[:,:2]-=center[:2]\n",
    "            px=pts[:,0]*w; py=pts[:,1]*h\n",
    "            scale=max(px.max()-px.min(), py.max()-py.min(), 1e-3)\n",
    "            pts[:,:2]/=(scale/max(w,h))\n",
    "            return pts.flatten().astype(np.float32)\n",
    "        L=normalize_hand(left,w,h); R=normalize_hand(right,w,h)\n",
    "        pres=np.array([1.0 if left else 0.0, 1.0 if right else 0.0], dtype=np.float32)\n",
    "        feats.append(np.concatenate([L,R,pres]))\n",
    "        i+=1\n",
    "        if len(feats)>=seq_len: break\n",
    "    cap.release()\n",
    "    hands.close()\n",
    "    if len(feats)<seq_len: return None\n",
    "    return np.stack(feats[:seq_len], axis=0)\n",
    "\n",
    "model = tf.keras.models.load_model(\"bisindo_landmarks.keras\")\n",
    "labels = json.load(open(\"cache_landmarks/labels.json\"))\n",
    "\n",
    "seq = extract_seq_from_video(\"D:/SFT_ATTEMPT2/raw_video/Duduk/Duduk_001.mp4\", seq_len=20)\n",
    "if seq is None:\n",
    "    print(\"Not enough frames or missing landmarks\")\n",
    "else:\n",
    "    pred = model.predict(np.expand_dims(seq,0))[0]\n",
    "    print(\"Top probs:\", sorted([(labels[i], float(pred[i])) for i in range(len(pred))], key=lambda x:-x[1])[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aded3844",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088888f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (250, 20, 96, 96, 3) (250,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "DATASET_DIR = \"D:/SFT_ATTEMPT2/raw_video\"  # <-- adjust full path to your dataset\n",
    "TARGET_WORDS = [\"Halo\", \"Kamu\", \"Apa\", \"Dimana\", \"Duduk\"]\n",
    "IMG_SIZE = 96\n",
    "FRAMES_PER_VIDEO = 20  # sample fixed length from each video\n",
    "\n",
    "def load_videos():\n",
    "    X, y = [], []\n",
    "    for word in TARGET_WORDS:\n",
    "        word_path = os.path.join(DATASET_DIR, word)\n",
    "        for vid_file in os.listdir(word_path):\n",
    "            vid_path = os.path.join(word_path, vid_file)\n",
    "            cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "            frames = []\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = frame.astype(\"float32\") / 255.0\n",
    "                frames.append(frame)\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "            # Uniformly sample FRAMES_PER_VIDEO frames\n",
    "            if len(frames) >= FRAMES_PER_VIDEO:\n",
    "                idxs = np.linspace(0, len(frames)-1, FRAMES_PER_VIDEO, dtype=int)\n",
    "                frames = [frames[i] for i in idxs]\n",
    "                X.append(frames)\n",
    "                y.append(TARGET_WORDS.index(word))\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = load_videos()\n",
    "print(\"Shape:\", X.shape, y.shape)  # (num_samples, FRAMES_PER_VIDEO, 96, 96, 3)\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8772a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30976</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">15,925,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m896\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m64\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m30976\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m15,925,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,953,733</span> (60.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,953,733\u001b[0m (60.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,953,733</span> (60.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,953,733\u001b[0m (60.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "FRAMES_PER_VIDEO = X_train.shape[1]  # should be 20\n",
    "IMG_SIZE = X_train.shape[2]          # should be 96\n",
    "NUM_CLASSES = len(TARGET_WORDS)\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential([\n",
    "        # CNN applied to each frame (via TimeDistributed)\n",
    "        layers.TimeDistributed(layers.Conv2D(32, (3,3), activation=\"relu\"),\n",
    "                               input_shape=(FRAMES_PER_VIDEO, IMG_SIZE, IMG_SIZE, 3)),\n",
    "        layers.TimeDistributed(layers.MaxPooling2D((2,2))),\n",
    "        layers.TimeDistributed(layers.Conv2D(64, (3,3), activation=\"relu\")),\n",
    "        layers.TimeDistributed(layers.MaxPooling2D((2,2))),\n",
    "        layers.TimeDistributed(layers.Flatten()),\n",
    "\n",
    "        # LSTM across frames\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e067a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 2s/step - accuracy: 0.1999 - loss: 1.6835 - val_accuracy: 0.5200 - val_loss: 1.1038\n",
      "Epoch 2/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step - accuracy: 0.7295 - loss: 0.8012 - val_accuracy: 1.0000 - val_loss: 0.1872\n",
      "Epoch 3/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - accuracy: 0.9806 - loss: 0.1604 - val_accuracy: 1.0000 - val_loss: 0.0254\n",
      "Epoch 4/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0171 - val_accuracy: 1.0000 - val_loss: 0.0070\n",
      "Epoch 5/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0033\n",
      "Epoch 6/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
      "Epoch 7/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
      "Epoch 8/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 9/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 9.4470e-04 - val_accuracy: 1.0000 - val_loss: 8.4649e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 7.2578e-04 - val_accuracy: 1.0000 - val_loss: 6.6412e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 5.7410e-04 - val_accuracy: 1.0000 - val_loss: 5.1778e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 4.3134e-04 - val_accuracy: 1.0000 - val_loss: 4.1712e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 3.5681e-04 - val_accuracy: 1.0000 - val_loss: 3.3832e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 3.0332e-04 - val_accuracy: 1.0000 - val_loss: 2.7400e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.5872e-04 - val_accuracy: 1.0000 - val_loss: 2.4706e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.1978e-04 - val_accuracy: 1.0000 - val_loss: 2.0943e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 2.1590e-04 - val_accuracy: 1.0000 - val_loss: 1.8799e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.5227e-04 - val_accuracy: 1.0000 - val_loss: 1.6393e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.5951e-04 - val_accuracy: 1.0000 - val_loss: 1.9949e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 1.6279e-04 - val_accuracy: 1.0000 - val_loss: 1.3809e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb68eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"bisindo_words_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4b6fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = TARGET_WORDS  # same list you used for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25dc6f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Halo       1.00      1.00      1.00        13\n",
      "        Kamu       1.00      1.00      1.00         7\n",
      "         Apa       1.00      1.00      1.00        10\n",
      "      Dimana       1.00      1.00      1.00        11\n",
      "       Duduk       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "[[13  0  0  0  0]\n",
      " [ 0  7  0  0  0]\n",
      " [ 0  0 10  0  0]\n",
      " [ 0  0  0 11  0]\n",
      " [ 0  0  0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# If y_val is already integer encoded, just use it directly\n",
    "y_true = y_val  \n",
    "\n",
    "# Report\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d449e1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 Starting real-time gesture recognition. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"bisindo_words_model.h5\")\n",
    "class_names = [\"Halo\", \"Kamu\", \"Apa\", \"Dimana\", \"Duduk\"]\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = 96\n",
    "FRAMES_PER_VIDEO = 20\n",
    "CONF_THRESHOLD = 0.6  # adjust as needed\n",
    "\n",
    "# Buffer to store recent frames\n",
    "frame_buffer = deque(maxlen=FRAMES_PER_VIDEO)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = frame.astype(\"float32\") / 255.0\n",
    "    return frame\n",
    "\n",
    "def predict_sequence(frames):\n",
    "    X_input = np.expand_dims(np.array(frames), axis=0)  # (1, 20, 96, 96, 3)\n",
    "    probs = model.predict(X_input, verbose=0)\n",
    "    pred_idx = np.argmax(probs, axis=1)[0]\n",
    "    confidence = np.max(probs)\n",
    "    if confidence < CONF_THRESHOLD:\n",
    "        return \"No Gesture\", confidence\n",
    "    return class_names[pred_idx], confidence\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"🎥 Starting real-time gesture recognition. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess and store frame\n",
    "    processed = preprocess_frame(frame)\n",
    "    frame_buffer.append(processed)\n",
    "\n",
    "    prediction_text = \"Waiting...\"\n",
    "    conf = 0\n",
    "\n",
    "    # Once we have enough frames, predict\n",
    "    if len(frame_buffer) == FRAMES_PER_VIDEO:\n",
    "        pred, conf = predict_sequence(list(frame_buffer))\n",
    "        prediction_text = f\"{pred} ({conf:.2f})\"\n",
    "\n",
    "    # Show on screen\n",
    "    cv2.putText(frame, prediction_text, (30, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Real-Time BISINDO\", frame)\n",
    "\n",
    "    # Quit key\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbbef8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects input shape: (None, 20, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"bisindo_words_model.h5\")\n",
    "class_names = [\"Halo\", \"Kamu\", \"Apa\", \"Dimana\", \"Duduk\"]\n",
    "\n",
    "# Parameters\n",
    "SEQ_LENGTH = 30\n",
    "CONF_THRESHOLD = 0.8\n",
    "sentence = []\n",
    "pred_buffer = deque(maxlen=5)\n",
    "\n",
    "# MediaPipe hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Detect model expected input shape\n",
    "model_input_shape = model.input_shape  # e.g. (None, 30, 63) or (None, 1890)\n",
    "print(\"Model expects input shape:\", model_input_shape)\n",
    "\n",
    "def format_input(keypoints):\n",
    "    \"\"\"Format landmark array based on model input shape\"\"\"\n",
    "    flat = np.array(keypoints).flatten()  # shape (63,)\n",
    "    if len(model_input_shape) == 3:\n",
    "        # Shape (batch, timesteps, features)\n",
    "        timesteps, feat = model_input_shape[1], model_input_shape[2]\n",
    "        frame_data = flat.reshape(1, 1, -1)        # (1, 1, 63)\n",
    "        return np.repeat(frame_data, timesteps, axis=1)  # (1, 30, 63)\n",
    "    elif len(model_input_shape) == 2:\n",
    "        # Shape (batch, features)\n",
    "        return flat.reshape(1, -1)  # (1, 1890)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported input shape: {model_input_shape}\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            keypoints = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                keypoints.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "            # Prepare input for model\n",
    "            try:\n",
    "                input_data = format_input(keypoints)\n",
    "                preds = model.predict(input_data, verbose=0)[0]\n",
    "                confidence = np.max(preds)\n",
    "                predicted_label = class_names[np.argmax(preds)]\n",
    "\n",
    "                # Only accept confident predictions\n",
    "                if confidence > CONF_THRESHOLD:\n",
    "                    pred_buffer.append(predicted_label)\n",
    "                    if len(pred_buffer) == pred_buffer.maxlen and all(p == pred_buffer[0] for p in pred_buffer):\n",
    "                        sentence.append(predicted_label)\n",
    "                        pred_buffer.clear()\n",
    "            except Exception as e:\n",
    "                cv2.putText(frame, f\"ERR {str(e)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2)\n",
    "\n",
    "    # Display prediction and sentence\n",
    "    if sentence:\n",
    "        cv2.putText(frame, \"Sentence: \" + \" \".join(sentence[-10:]), (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Real-Time Sign Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c41c3d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "\n",
    "# -------------------\n",
    "# Setup\n",
    "# -------------------\n",
    "SEQUENCE_LENGTH = 20\n",
    "frame_buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=2,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Load your model + labels\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"bisindo_words_model.h5\")\n",
    "labels = [\"apa\", \"dimana\", \"kamu\", \"halo\", \"duduk\", \"NO\"]  # adjust to your dataset\n",
    "\n",
    "# State variables\n",
    "sentence = []\n",
    "last_label = None\n",
    "same_count = 0\n",
    "CONF_THRESH = 0.7\n",
    "STABILITY_COUNT = 5\n",
    "\n",
    "prev_frame_gray = None\n",
    "\n",
    "# -------------------\n",
    "# Main Loop\n",
    "# -------------------\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # -------------------\n",
    "    # Hand presence detection\n",
    "    # -------------------\n",
    "    hand_present = results.multi_hand_landmarks is not None\n",
    "    if hand_present:\n",
    "        for lm in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, lm, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # -------------------\n",
    "    # Motion detection\n",
    "    # -------------------\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    motion_val = 0.0\n",
    "    if prev_frame_gray is not None:\n",
    "        diff = cv2.absdiff(prev_frame_gray, gray)\n",
    "        motion_val = np.mean(diff) / 255.0\n",
    "    prev_frame_gray = gray\n",
    "\n",
    "    # -------------------\n",
    "    # Preprocess and buffer frames\n",
    "    # -------------------\n",
    "    img = cv2.resize(frame, (96, 96))\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "    frame_buffer.append(img)\n",
    "\n",
    "    accepted_label, accepted_conf = \"NO\", 1.0\n",
    "\n",
    "    # -------------------\n",
    "    # Prediction (only if we have 20 frames + hands detected)\n",
    "    # -------------------\n",
    "    if len(frame_buffer) == SEQUENCE_LENGTH:\n",
    "        input_data = np.expand_dims(frame_buffer, axis=0)  # (1,20,96,96,3)\n",
    "        preds = model.predict(input_data, verbose=0)[0]\n",
    "\n",
    "        pred_idx = np.argmax(preds)\n",
    "        pred_conf = preds[pred_idx]\n",
    "        pred_label = labels[pred_idx]\n",
    "\n",
    "        # Decide if it's \"NO gesture\"\n",
    "        if not hand_present or motion_val < 0.01:\n",
    "            accepted_label = \"NO\"\n",
    "            accepted_conf = 1.0\n",
    "        elif pred_conf >= CONF_THRESH:\n",
    "            if pred_label == last_label:\n",
    "                same_count += 1\n",
    "            else:\n",
    "                same_count = 1\n",
    "                last_label = pred_label\n",
    "\n",
    "            if same_count >= STABILITY_COUNT:\n",
    "                accepted_label = pred_label\n",
    "                accepted_conf = pred_conf\n",
    "                sentence.append(pred_label)\n",
    "\n",
    "    # -------------------\n",
    "    # Display\n",
    "    # -------------------\n",
    "    cv2.putText(frame, f\"Pred: {accepted_label} {accepted_conf:.2f}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, \"Sentence: \" + \" \".join(sentence[-5:]), (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "    cv2.putText(frame, f\"Motion:{motion_val:.4f} Hands:{int(hand_present)}\", (10, 90),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Sign Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e76e84af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# ---- Load trained model ----\n",
    "model = tf.keras.models.load_model(\"bisindo_words_model.h5\")\n",
    "class_names = [\"Halo\", \"Kamu\", \"Apa\", \"Dimana\", \"Duduk\"]\n",
    "\n",
    "# ---- Parameters ----\n",
    "SEQ_LENGTH = 20   # your model expects 20 frames\n",
    "sentence = []\n",
    "pred_buffer = deque(maxlen=5)   # for stable final labels\n",
    "prob_buffer = deque(maxlen=10)  # smooth probabilities\n",
    "\n",
    "# Running averages for auto-threshold\n",
    "class_running_mean = {cls: 0.5 for cls in class_names}\n",
    "alpha = 0.1        # smoothing (0.1 = slow, 0.3 = fast)\n",
    "base_margin = 0.1  # margin above average confidence\n",
    "NO_GESTURE_LABEL = \"NoGesture\"\n",
    "\n",
    "# ---- MediaPipe setup ----\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # ---- Extract hand ROI ----\n",
    "            h, w, _ = frame.shape\n",
    "            x_coords = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y_coords = [lm.y for lm in hand_landmarks.landmark]\n",
    "            x_min, x_max = int(min(x_coords) * w), int(max(x_coords) * w)\n",
    "            y_min, y_max = int(min(y_coords) * h), int(max(y_coords) * h)\n",
    "\n",
    "            # Add padding\n",
    "            x_min, y_min = max(0, x_min - 20), max(0, y_min - 20)\n",
    "            x_max, y_max = min(w, x_max + 20), min(h, y_max + 20)\n",
    "\n",
    "            roi = frame[y_min:y_max, x_min:x_max]\n",
    "            if roi.size == 0:\n",
    "                continue\n",
    "            roi_resized = cv2.resize(roi, (96, 96))\n",
    "            roi_norm = roi_resized.astype(\"float32\") / 255.0\n",
    "\n",
    "            # ---- Format input (1,20,96,96,3) ----\n",
    "            input_data = np.expand_dims(roi_norm, axis=0)   # (1,96,96,3)\n",
    "            input_data = np.repeat(input_data[np.newaxis, ...], SEQ_LENGTH, axis=1)\n",
    "\n",
    "            # ---- Prediction ----\n",
    "            preds = model.predict(input_data, verbose=0)[0]\n",
    "            prob_buffer.append(preds)\n",
    "\n",
    "            # Average probs across buffer\n",
    "            avg_preds = np.mean(prob_buffer, axis=0)\n",
    "\n",
    "            # Top-2 predictions\n",
    "            sorted_idx = np.argsort(avg_preds)[::-1]\n",
    "            top1, top2 = sorted_idx[0], sorted_idx[1]\n",
    "            pred_idx = top1\n",
    "            confidence = avg_preds[pred_idx]\n",
    "            predicted_label = class_names[pred_idx]\n",
    "\n",
    "            # ---- Auto-adjust threshold ----\n",
    "            prev_mean = class_running_mean[predicted_label]\n",
    "            new_mean = (1 - alpha) * prev_mean + alpha * confidence\n",
    "            class_running_mean[predicted_label] = new_mean\n",
    "\n",
    "            dynamic_thresh = new_mean + base_margin\n",
    "\n",
    "            # ---- Decide final prediction ----\n",
    "            final_label = NO_GESTURE_LABEL\n",
    "            if confidence > dynamic_thresh:\n",
    "                pred_buffer.append(predicted_label)\n",
    "\n",
    "                if len(pred_buffer) == pred_buffer.maxlen and all(p == pred_buffer[0] for p in pred_buffer):\n",
    "                    sentence.append(predicted_label)\n",
    "                    pred_buffer.clear()\n",
    "                final_label = predicted_label\n",
    "\n",
    "            # ---- Debug text ----\n",
    "            debug_text = (f\"{class_names[top1]}:{avg_preds[top1]:.2f} (th:{dynamic_thresh:.2f}) | \"\n",
    "                          f\"{class_names[top2]}:{avg_preds[top2]:.2f}\")\n",
    "            cv2.putText(frame, debug_text, (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, f\"Final: {final_label}\", (10, 60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "    # ---- Show sentence ----\n",
    "    if sentence:\n",
    "        cv2.putText(frame, \"Sentence: \" + \" \".join(sentence[-10:]), (10, 90),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (200, 100, 200), 2)\n",
    "\n",
    "    cv2.imshow(\"Real-Time Sign Recognition\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d13ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Halo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:55<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Kamu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:39<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Apa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:33<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Dimana\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:45<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 videos for Duduk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:20<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset shapes: (773, 20, 128) (773,)\n",
      "Saved to cache_landmarks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess.py\n",
    "import os, glob, json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_DIR = \"D:/SFT_ATTEMPT2/raw_video\"\n",
    "TARGET_WORDS = [\"Halo\",\"Kamu\",\"Apa\",\"Dimana\",\"Duduk\"]\n",
    "CACHE_DIR = \"cache_landmarks\"\n",
    "SEQ_LEN = 20\n",
    "TARGET_FPS = 24\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, model_complexity=0,\n",
    "                       max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "def normalize_hand(landmarks, w, h):\n",
    "    \"\"\"Return 63-d vector for 21 landmarks (x,y,z) normalized relative to wrist & scale.\n",
    "       If landmarks is None return zeros.\"\"\"\n",
    "    if landmarks is None:\n",
    "        return np.zeros(63, dtype=np.float32)\n",
    "    pts = np.array([(lm.x, lm.y, lm.z) for lm in landmarks], dtype=np.float32)\n",
    "    # center at wrist (index 0)\n",
    "    center = pts[0].copy()\n",
    "    pts[:, :2] -= center[:2]\n",
    "    px = pts[:,0]*w\n",
    "    py = pts[:,1]*h\n",
    "    scale = max(px.max()-px.min(), py.max()-py.min(), 1e-3)\n",
    "    pts[:, :2] /= (scale / max(w,h))\n",
    "    return pts.flatten().astype(np.float32)\n",
    "\n",
    "def extract_sequences_from_video(path, seq_len=SEQ_LEN, target_fps=TARGET_FPS):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        return []\n",
    "    src_fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    step = max(int(round(src_fps / target_fps)), 1)\n",
    "\n",
    "    frames_feats = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if i % step != 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        h,w = frame.shape[:2]\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = hands.process(rgb)\n",
    "\n",
    "        left = None; right = None\n",
    "        if res.multi_hand_landmarks and res.multi_handedness:\n",
    "            # pair them\n",
    "            for lm, handed in zip(res.multi_hand_landmarks, res.multi_handedness):\n",
    "                if handed.classification[0].label.lower() == \"left\":\n",
    "                    left = lm.landmark\n",
    "                else:\n",
    "                    right = lm.landmark\n",
    "\n",
    "        L = normalize_hand(left, w, h)\n",
    "        R = normalize_hand(right, w, h)\n",
    "        present = np.array([1.0 if left is not None else 0.0, 1.0 if right is not None else 0.0], dtype=np.float32)\n",
    "        feat = np.concatenate([L, R, present])    # length = 63+63+2 = 128\n",
    "        frames_feats.append(feat)\n",
    "        i += 1\n",
    "\n",
    "    cap.release()\n",
    "    if len(frames_feats) < seq_len:\n",
    "        return []\n",
    "\n",
    "    seqs = []\n",
    "    for start in range(0, len(frames_feats) - seq_len + 1, seq_len):\n",
    "        seqs.append(np.stack(frames_feats[start:start+seq_len], axis=0))\n",
    "    return seqs\n",
    "\n",
    "# Scan dataset\n",
    "X=[]; y=[]\n",
    "label_map = {w:i for i,w in enumerate(TARGET_WORDS)}\n",
    "for w in TARGET_WORDS:\n",
    "    folder = os.path.join(DATASET_DIR, w)\n",
    "    if not os.path.isdir(folder): \n",
    "        print(\"Missing folder:\", folder); continue\n",
    "    vids = glob.glob(os.path.join(folder, \"*.mp4\"))\n",
    "    print(f\"Found {len(vids)} videos for {w}\")\n",
    "    for v in tqdm(vids):\n",
    "        seqs = extract_sequences_from_video(v)\n",
    "        for s in seqs:\n",
    "            X.append(s)   # shape (T,128)\n",
    "            y.append(label_map[w])\n",
    "\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.int64)\n",
    "\n",
    "print(\"Built dataset shapes:\", X.shape, y.shape)\n",
    "np.save(os.path.join(CACHE_DIR, \"X.npy\"), X)\n",
    "np.save(os.path.join(CACHE_DIR, \"y.npy\"), y)\n",
    "with open(os.path.join(CACHE_DIR, \"labels.json\"), \"w\") as f:\n",
    "    json.dump(TARGET_WORDS, f)\n",
    "print(\"Saved to\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84dcd400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shapes: (773, 20, 128) (773,)\n",
      "Train: (618, 20, 128) (618,) Val: (155, 20, 128) (155,)\n",
      "Class weights: {0: 0.8524137931034482, 1: 1.0747826086956522, 2: 1.0564102564102564, 3: 1.03, 4: 1.0214876033057851}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_81\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_81\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ masking_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ any_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ any_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_1 (\u001b[38;5;33mMasking\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any_1 (\u001b[38;5;33mAny\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m131,584\u001b[0m │ masking_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ any_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m49,408\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ any_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m325\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,477</span> (724.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m185,477\u001b[0m (724.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,477</span> (724.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m185,477\u001b[0m (724.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 20\n",
      "Epoch 1/80\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.81290, saving model to bisindo_landmarks.keras\n",
      "20/20 - 5s - 254ms/step - accuracy: 0.6440 - loss: 1.1415 - val_accuracy: 0.8129 - val_loss: 0.6547 - learning_rate: 0.0010\n",
      "Epoch 2/80\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.81290 to 0.87097, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.8738 - loss: 0.4228 - val_accuracy: 0.8710 - val_loss: 0.3642 - learning_rate: 0.0010\n",
      "Epoch 3/80\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.87097 to 0.89032, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 21ms/step - accuracy: 0.9207 - loss: 0.2708 - val_accuracy: 0.8903 - val_loss: 0.3343 - learning_rate: 0.0010\n",
      "Epoch 4/80\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.89032 to 0.89677, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 23ms/step - accuracy: 0.9482 - loss: 0.1781 - val_accuracy: 0.8968 - val_loss: 0.2977 - learning_rate: 0.0010\n",
      "Epoch 5/80\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.89677 to 0.90323, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 24ms/step - accuracy: 0.9466 - loss: 0.1700 - val_accuracy: 0.9032 - val_loss: 0.2771 - learning_rate: 0.0010\n",
      "Epoch 6/80\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.90323 to 0.90968, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9595 - loss: 0.1518 - val_accuracy: 0.9097 - val_loss: 0.2695 - learning_rate: 0.0010\n",
      "Epoch 7/80\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.90968\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9466 - loss: 0.1675 - val_accuracy: 0.8968 - val_loss: 0.3032 - learning_rate: 0.0010\n",
      "Epoch 8/80\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.90968\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9191 - loss: 0.2215 - val_accuracy: 0.9097 - val_loss: 0.2713 - learning_rate: 0.0010\n",
      "Epoch 9/80\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.90968\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9612 - loss: 0.1234 - val_accuracy: 0.9032 - val_loss: 0.2566 - learning_rate: 0.0010\n",
      "Epoch 10/80\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.90968 to 0.92258, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9693 - loss: 0.0999 - val_accuracy: 0.9226 - val_loss: 0.2243 - learning_rate: 0.0010\n",
      "Epoch 11/80\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.92258\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9628 - loss: 0.0965 - val_accuracy: 0.9097 - val_loss: 0.2329 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\n",
      "Epoch 12: val_accuracy improved from 0.92258 to 0.92903, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9660 - loss: 0.0897 - val_accuracy: 0.9290 - val_loss: 0.2167 - learning_rate: 0.0010\n",
      "Epoch 13/80\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.92903 to 0.93548, saving model to bisindo_landmarks.keras\n",
      "20/20 - 0s - 21ms/step - accuracy: 0.9741 - loss: 0.0815 - val_accuracy: 0.9355 - val_loss: 0.2169 - learning_rate: 0.0010\n",
      "Epoch 14/80\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9693 - loss: 0.0835 - val_accuracy: 0.9355 - val_loss: 0.2127 - learning_rate: 0.0010\n",
      "Epoch 15/80\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 19ms/step - accuracy: 0.9725 - loss: 0.0819 - val_accuracy: 0.9355 - val_loss: 0.2096 - learning_rate: 0.0010\n",
      "Epoch 16/80\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 19ms/step - accuracy: 0.9741 - loss: 0.0769 - val_accuracy: 0.9161 - val_loss: 0.2291 - learning_rate: 0.0010\n",
      "Epoch 17/80\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9725 - loss: 0.0711 - val_accuracy: 0.9290 - val_loss: 0.2355 - learning_rate: 0.0010\n",
      "Epoch 18/80\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.93548\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "20/20 - 0s - 21ms/step - accuracy: 0.9741 - loss: 0.0682 - val_accuracy: 0.9355 - val_loss: 0.2178 - learning_rate: 0.0010\n",
      "Epoch 19/80\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 21ms/step - accuracy: 0.9757 - loss: 0.0614 - val_accuracy: 0.9355 - val_loss: 0.1991 - learning_rate: 5.0000e-04\n",
      "Epoch 20/80\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9757 - loss: 0.0651 - val_accuracy: 0.9355 - val_loss: 0.2151 - learning_rate: 5.0000e-04\n",
      "Epoch 21/80\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9757 - loss: 0.0638 - val_accuracy: 0.9355 - val_loss: 0.2116 - learning_rate: 5.0000e-04\n",
      "Epoch 22/80\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.93548\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9741 - loss: 0.0610 - val_accuracy: 0.9355 - val_loss: 0.2185 - learning_rate: 5.0000e-04\n",
      "Epoch 23/80\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 19ms/step - accuracy: 0.9773 - loss: 0.0561 - val_accuracy: 0.9355 - val_loss: 0.2194 - learning_rate: 2.5000e-04\n",
      "Epoch 24/80\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 22ms/step - accuracy: 0.9741 - loss: 0.0603 - val_accuracy: 0.9290 - val_loss: 0.2128 - learning_rate: 2.5000e-04\n",
      "Epoch 25/80\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.93548\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9741 - loss: 0.0598 - val_accuracy: 0.9290 - val_loss: 0.2164 - learning_rate: 2.5000e-04\n",
      "Epoch 26/80\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 19ms/step - accuracy: 0.9838 - loss: 0.0529 - val_accuracy: 0.9290 - val_loss: 0.2144 - learning_rate: 1.2500e-04\n",
      "Epoch 27/80\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.93548\n",
      "20/20 - 0s - 20ms/step - accuracy: 0.9773 - loss: 0.0556 - val_accuracy: 0.9290 - val_loss: 0.2110 - learning_rate: 1.2500e-04\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Training finished. Model saved to bisindo_landmarks.keras\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "CACHE_DIR = \"cache_landmarks\"\n",
    "SEQ_LEN = 20\n",
    "MODEL_PATH = \"bisindo_landmarks.keras\"   # recommended Keras v3 format\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 80\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "X = np.load(os.path.join(CACHE_DIR, \"X.npy\"))  # expected shape (N, T, F)\n",
    "y = np.load(os.path.join(CACHE_DIR, \"y.npy\"))\n",
    "with open(os.path.join(CACHE_DIR, \"labels.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "print(\"Loaded data shapes:\", X.shape, y.shape)\n",
    "num_classes = len(labels)\n",
    "\n",
    "# -------------------------\n",
    "# Train / val split\n",
    "# -------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train:\", X_train.shape, y_train.shape, \"Val:\", X_val.shape, y_val.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Augmentation helper\n",
    "# -------------------------\n",
    "def augment_batch(X_batch, prob=0.5):\n",
    "    \"\"\"Add small gaussian jitter to landmarks with probability per sample.\"\"\"\n",
    "    Xb = X_batch.copy()\n",
    "    for i in range(len(Xb)):\n",
    "        if np.random.rand() < prob:\n",
    "            noise = np.random.normal(0, 0.01, Xb[i].shape).astype(np.float32)\n",
    "            Xb[i] = Xb[i] + noise\n",
    "    return Xb\n",
    "\n",
    "# -------------------------\n",
    "# Compute class weights and map to sample weights\n",
    "# -------------------------\n",
    "class_weights_arr = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights_arr))\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# -------------------------\n",
    "# Generator that yields (Xb, yb, sample_weight)\n",
    "# -------------------------\n",
    "def gen_with_weights(Xa, ya, batch_size=BATCH_SIZE):\n",
    "    n = len(Xa)\n",
    "    idx = np.arange(n)\n",
    "    while True:\n",
    "        np.random.shuffle(idx)\n",
    "        for i in range(0, n, batch_size):\n",
    "            b = idx[i:i+batch_size]\n",
    "            Xb = Xa[b].copy()\n",
    "            Xb = augment_batch(Xb, prob=0.5)\n",
    "            yb = ya[b]\n",
    "            # sample weight per item based on its class\n",
    "            sw = np.array([class_weights[int(lbl)] for lbl in yb], dtype=np.float32)\n",
    "            yield Xb, yb, sw\n",
    "\n",
    "# -------------------------\n",
    "# Build model (LSTM on landmarks)\n",
    "# -------------------------\n",
    "input_shape = X_train.shape[1:]  # (T, F)\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = keras.layers.Masking(mask_value=0.0)(inputs)\n",
    "x = keras.layers.LSTM(128, return_sequences=True)(x)\n",
    "x = keras.layers.Dropout(0.25)(x)\n",
    "x = keras.layers.LSTM(64)(x)\n",
    "x = keras.layers.Dropout(0.25)(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# Callbacks\n",
    "# -------------------------\n",
    "cb = [\n",
    "    keras.callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor=\"val_accuracy\", mode=\"max\", verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Fit using generator that yields sample weights\n",
    "# -------------------------\n",
    "steps_per_epoch = max(1, math.ceil(len(X_train) / BATCH_SIZE))\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "\n",
    "history = model.fit(\n",
    "    gen_with_weights(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cb,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ensure final save in Keras format\n",
    "model.save(MODEL_PATH)\n",
    "print(\"Training finished. Model saved to\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26c8b873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough frames or missing landmarks\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np, json, tensorflow as tf\n",
    "from pathlib import Path\n",
    "# assumes you have the normalize function from preprocess.py (or reuse code)\n",
    "\n",
    "# quick helper: extract first seq of length SEQ_LEN from a video using mediapipe landmarks\n",
    "def extract_seq_from_video(video_path, seq_len=20, target_fps=24):\n",
    "    import mediapipe as mp\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2,\n",
    "                           min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    feats=[]\n",
    "    i=0\n",
    "    src_fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    step = max(int(round(src_fps/target_fps)),1)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        if i % step != 0:\n",
    "            i+=1; continue\n",
    "        h,w = frame.shape[:2]\n",
    "        res = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        left=None; right=None\n",
    "        if res.multi_hand_landmarks and res.multi_handedness:\n",
    "            for lm, handed in zip(res.multi_hand_landmarks, res.multi_handedness):\n",
    "                if handed.classification[0].label.lower()==\"left\": left=lm.landmark\n",
    "                else: right=lm.landmark\n",
    "        # use normalize_hand function from preprocess.py (copy it here)\n",
    "        def normalize_hand(landmarks,w,h):\n",
    "            if landmarks is None: return np.zeros(63,dtype=np.float32)\n",
    "            pts=np.array([(lm.x,lm.y,lm.z) for lm in landmarks],dtype=np.float32)\n",
    "            center=pts[0].copy(); pts[:,:2]-=center[:2]\n",
    "            px=pts[:,0]*w; py=pts[:,1]*h\n",
    "            scale=max(px.max()-px.min(), py.max()-py.min(), 1e-3)\n",
    "            pts[:,:2]/=(scale/max(w,h))\n",
    "            return pts.flatten().astype(np.float32)\n",
    "        L=normalize_hand(left,w,h); R=normalize_hand(right,w,h)\n",
    "        pres=np.array([1.0 if left else 0.0, 1.0 if right else 0.0], dtype=np.float32)\n",
    "        feats.append(np.concatenate([L,R,pres]))\n",
    "        i+=1\n",
    "        if len(feats)>=seq_len: break\n",
    "    cap.release()\n",
    "    hands.close()\n",
    "    if len(feats)<seq_len: return None\n",
    "    return np.stack(feats[:seq_len], axis=0)\n",
    "\n",
    "model = tf.keras.models.load_model(\"bisindo_landmarks.keras\")\n",
    "labels = json.load(open(\"cache_landmarks/labels.json\"))\n",
    "\n",
    "seq = extract_seq_from_video(\"D:/SFT_ATTEMPT2/raw_video/Duduk/Duduk_001.mp4\", seq_len=20)\n",
    "if seq is None:\n",
    "    print(\"Not enough frames or missing landmarks\")\n",
    "else:\n",
    "    pred = model.predict(np.expand_dims(seq,0))[0]\n",
    "    print(\"Top probs:\", sorted([(labels[i], float(pred[i])) for i in range(len(pred))], key=lambda x:-x[1])[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
